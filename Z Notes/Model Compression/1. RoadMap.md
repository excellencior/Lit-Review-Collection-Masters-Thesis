# 1) Core pillars (what to master first)

- **Pruning & sparsity:** classical unstructured vs. hardware-friendly structured/N:M; lottery-ticket ideas for rewinding/retraining. ([SCIRP](https://www.scirp.org/reference/referencespapers?referenceid=2993907&utm_source=chatgpt.com "Han, S., Mao, H. and Dally, W.J. (2016) Deep Compression ..."), [DSpace](https://dspace.mit.edu/bitstream/handle/1721.1/129953/1803.03635.pdf?isAllowed=y&sequence=2&utm_source=chatgpt.com "[PDF] MIT Open Access Articles The lottery ticket hypothesis"))
    
- **Quantization:** PTQ vs. QAT; per-tensor/channel, groupwise, mixed precision; activation outliers; 8-/4-/3-/2-bit regimes; FP8 (today) and FP4 (emerging). For LLMs, learn GPTQ (PTQ 3–4b), LLM.int8(), and 4-bit finetuning via QLoRA. ([arXiv](https://arxiv.org/abs/2210.17323?utm_source=chatgpt.com "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"))
    
- **Knowledge distillation (KD):** logits/feature/attention-map distillation; small transformer students (DistilBERT, TinyBERT, MiniLM). ([Semantic Scholar](https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19?utm_source=chatgpt.com "Distilling the Knowledge in a Neural Network - Semantic Scholar"), [arXiv](https://arxiv.org/abs/1910.01108?utm_source=chatgpt.com "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and ..."))
    
- **Compact architectures & PEFT:** depth/width multipliers (EfficientNet), parameter sharing/adapters/LoRA; combine with quantization. (LoRA is in QLoRA above.) ([arXiv](https://arxiv.org/abs/2401.18079?utm_source=chatgpt.com "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"))
    
- **System stack:** deploy with TensorRT-LLM / vLLM; follow FP8/FP4 kernels and Blackwell-class GPUs for real perf. ([SemiAnalysis](https://semianalysis.com/2025/06/23/nvidia-tensor-core-evolution-from-volta-to-blackwell/?utm_source=chatgpt.com "NVIDIA Tensor Core Evolution: From Volta To Blackwell"), [Hugging Face](https://huggingface.co/papers/2405.16406?utm_source=chatgpt.com "SpinQuant: LLM quantization with learned rotations - Hugging Face"), [arXiv](https://arxiv.org/abs/2404.00456?utm_source=chatgpt.com "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"))
    

# 2) Practical toolchain (fast path)

- **Frameworks:** PyTorch (training), ONNX export, **TensorRT-LLM** for serving (quant+fusion), bitsandbytes (8/4-bit), GPTQ/AutoGPTQ (PTQ), vLLM (paged attention). Prioritize TensorRT-LLM because it’s aligned with FP8/FP4 roadmaps. ([SemiAnalysis](https://semianalysis.com/2025/06/23/nvidia-tensor-core-evolution-from-volta-to-blackwell/?utm_source=chatgpt.com "NVIDIA Tensor Core Evolution: From Volta To Blackwell"), [Hugging Face](https://huggingface.co/papers/2405.16406?utm_source=chatgpt.com "SpinQuant: LLM quantization with learned rotations - Hugging Face"))
    
- **Bench/eval habits:** report _accuracy/perplexity_, _latency (p50/p95)_, _throughput (tok/s, img/s)_, _VRAM/params_, and _energy_.
    

# 3) A crisp learning plan (≈12–16 weeks, adapt as needed)

**Phase A — Foundations (2–3 weeks)**  
Build intuition with small CNN/Transformer: magnitude pruning → structured pruning; PTQ (INT8) vs. QAT; run a tiny KD experiment. Read: Deep Compression + KD + Lottery Ticket. ([SCIRP](https://www.scirp.org/reference/referencespapers?referenceid=2993907&utm_source=chatgpt.com "Han, S., Mao, H. and Dally, W.J. (2016) Deep Compression ..."), [Semantic Scholar](https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19?utm_source=chatgpt.com "Distilling the Knowledge in a Neural Network - Semantic Scholar"), [DSpace](https://dspace.mit.edu/bitstream/handle/1721.1/129953/1803.03635.pdf?isAllowed=y&sequence=2&utm_source=chatgpt.com "[PDF] MIT Open Access Articles The lottery ticket hypothesis"))

**Phase B — Quantization that matters (3–4 weeks)**  
Implement **GPTQ** on an open LLM (e.g., 7B): compare 4-bit vs. 3-bit; add activation outlier handling; contrast with **LLM.int8()** and **QLoRA** finetuning on a downstream task. Deploy with TensorRT-LLM FP8 baseline for apples-to-apples latency. ([arXiv](https://arxiv.org/abs/2210.17323?utm_source=chatgpt.com "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"), [SemiAnalysis](https://semianalysis.com/2025/06/23/nvidia-tensor-core-evolution-from-volta-to-blackwell/?utm_source=chatgpt.com "NVIDIA Tensor Core Evolution: From Volta To Blackwell"))

**Phase C — Distillation + compression (3–4 weeks)**  
Train a distilled student (DistilBERT/TinyBERT/MiniLM style) and **then** quantize it; show Pareto gains vs. quantizing a large teacher directly. ([arXiv](https://arxiv.org/abs/1910.01108?utm_source=chatgpt.com "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and ..."))

**Phase D — Systems-grade serving (3–4 weeks)**  
Integrate with **TensorRT-LLM**; profile FP8 vs. INT8 vs. 4-bit weights; add **KV-cache quantization** (big memory win). Compare against a vLLM baseline. ([SemiAnalysis](https://semianalysis.com/2025/06/23/nvidia-tensor-core-evolution-from-volta-to-blackwell/?utm_source=chatgpt.com "NVIDIA Tensor Core Evolution: From Volta To Blackwell"), [NeurIPS Papers](https://papers.nips.cc/paper_files/paper/2024/file/028fcbcf85435d39a40c4d61b42c99a4-Paper-Conference.pdf?utm_source=chatgpt.com "[PDF] KVQuant: Towards 10 Million Context Length LLM Inference with KV ..."), [arXiv](https://arxiv.org/html/2402.02750v2?utm_source=chatgpt.com "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache - arXiv"))

# 4) Thesis directions (pick one and go deep)

1. **LLM PTQ beyond 4-bit + robust KV-cache quantization**
    
    - Goal: 3-/2-bit weights with minimal perplexity loss _and_ quantized KV cache.
        
    - Angle: rotation/outlier-aware grouping + data-free calibration; fuse with **KVQuant/KIVI-style** caches.
        
    - Why now: memory is the serving bottleneck; FP4/FP8 hardware is arriving—strong publishability. ([NeurIPS Papers](https://papers.nips.cc/paper_files/paper/2024/file/028fcbcf85435d39a40c4d61b42c99a4-Paper-Conference.pdf?utm_source=chatgpt.com "[PDF] KVQuant: Towards 10 Million Context Length LLM Inference with KV ..."), [arXiv](https://arxiv.org/html/2402.02750v2?utm_source=chatgpt.com "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache - arXiv"), [Hugging Face](https://huggingface.co/papers/2405.16406?utm_source=chatgpt.com "SpinQuant: LLM quantization with learned rotations - Hugging Face"))
        
2. **Hybrid “KD + Quant” students for domain tasks**
    
    - Goal: student ≤1–3B with KD, then 4-bit PTQ/QAT; beat teacher-PTQ on quality/latency per $/W.
        
    - Angle: attention-map KD + layerwise loss scheduling; then GPTQ/QAT with activation range learning. ([Semantic Scholar](https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19?utm_source=chatgpt.com "Distilling the Knowledge in a Neural Network - Semantic Scholar"), [arXiv](https://arxiv.org/abs/2210.17323?utm_source=chatgpt.com "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"))
        
3. **Structured sparsity co-design (N:M) + low-bit**
    
    - Goal: 2:4 (or similar) sparsity plus 8/4-bit on Tensor Cores; prove real speedups at batch≥1.
        
    - Angle: pruning during finetune + sparse kernels; report end-to-end gains, not just FLOPs. (Use TensorRT-LLM where possible.) ([SemiAnalysis](https://semianalysis.com/2025/06/23/nvidia-tensor-core-evolution-from-volta-to-blackwell/?utm_source=chatgpt.com "NVIDIA Tensor Core Evolution: From Volta To Blackwell"))
        
4. **Compression-aware serving**
    
    - Goal: couple compression with inference tricks (speculative decoding, paged attention) for cluster-level gains.
        
    - Use surveys below to scope gaps and metrics. ([arXiv](https://arxiv.org/abs/2404.14294?utm_source=chatgpt.com "A Survey on Efficient Inference for Large Language Models - arXiv"))
        

# 5) Evaluation recipe (what reviewers expect)

- **Baselines:** FP16/FP8 full-precision, INT8 PTQ, GPTQ-4b, QLoRA-4b. ([arXiv](https://arxiv.org/abs/2210.17323?utm_source=chatgpt.com "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"))
    
- **Datasets/benchmarks:** for LLMs (perplexity on WikiText2/PG19; zero-shot on commonsense tasks); for encoders (GLUE/SQuAD); for vision (ImageNet top-1).
    
- **Ablations:** calibration size vs. quality; bit-mixing (W4A8, W3A8); KV-cache bitwidth vs. throughput; distill loss types.
    
- **Systems:** latency/throughput on a fixed GPU; VRAM vs. context length; energy (nvidia-smi logging).
    

# 6) What to read (short, high-impact list)

- **Fundamentals:**  
    Deep Compression (pruning+quant+Huffman). ([SCIRP](https://www.scirp.org/reference/referencespapers?referenceid=2993907&utm_source=chatgpt.com "Han, S., Mao, H. and Dally, W.J. (2016) Deep Compression ..."))  
    Knowledge Distillation (Hinton et al.). ([Semantic Scholar](https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19?utm_source=chatgpt.com "Distilling the Knowledge in a Neural Network - Semantic Scholar"))  
    Lottery Ticket Hypothesis. ([DSpace](https://dspace.mit.edu/bitstream/handle/1721.1/129953/1803.03635.pdf?isAllowed=y&sequence=2&utm_source=chatgpt.com "[PDF] MIT Open Access Articles The lottery ticket hypothesis"))
    
- **LLM quantization (practical):**  
    **GPTQ** (3–4b PTQ). ([arXiv](https://arxiv.org/abs/2210.17323?utm_source=chatgpt.com "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"))  
    **LLM.int8()** (8-bit inference). ([arXiv](https://arxiv.org/abs/2404.00456?utm_source=chatgpt.com "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"))  
    **QLoRA** (4-bit finetuning via nf4 + LoRA). ([arXiv](https://arxiv.org/abs/2401.18079?utm_source=chatgpt.com "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"))  
    **KV cache quantization:** KVQuant (NeurIPS 2024), KIVI (2-bit). ([NeurIPS Papers](https://papers.nips.cc/paper_files/paper/2024/file/028fcbcf85435d39a40c4d61b42c99a4-Paper-Conference.pdf?utm_source=chatgpt.com "[PDF] KVQuant: Towards 10 Million Context Length LLM Inference with KV ..."), [arXiv](https://arxiv.org/html/2402.02750v2?utm_source=chatgpt.com "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache - arXiv"))
    
- **Systems & surveys (to frame the thesis):**  
    Efficient LLM inference (survey). ([arXiv](https://arxiv.org/abs/2404.14294?utm_source=chatgpt.com "A Survey on Efficient Inference for Large Language Models - arXiv"))  
    LLM compression/inference surveys (2024–2025). ([arXiv](https://arxiv.org/abs/2402.09748?utm_source=chatgpt.com "Model Compression and Efficient Inference for Large Language ..."))  
    **TensorRT-LLM** docs (deploy on FP8/FP4 hardware). ([SemiAnalysis](https://semianalysis.com/2025/06/23/nvidia-tensor-core-evolution-from-volta-to-blackwell/?utm_source=chatgpt.com "NVIDIA Tensor Core Evolution: From Volta To Blackwell"))
    
- **Hardware direction:**  
    **FP8→FP4 trajectory** on NVIDIA Blackwell; design your experiments to be “FP4-ready.” ([Hugging Face](https://huggingface.co/papers/2405.16406?utm_source=chatgpt.com "SpinQuant: LLM quantization with learned rotations - Hugging Face"), [arXiv](https://arxiv.org/abs/2404.00456?utm_source=chatgpt.com "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"))
    

# 7) Minimal hands-on milestones (for your thesis logbook)

- **M1:** Reproduce GPTQ-4b on a 7B LLM; match published perplexity; full latency/VRAM report. ([arXiv](https://arxiv.org/abs/2210.17323?utm_source=chatgpt.com "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"))
    
- **M2:** Add KV-cache quant (e.g., 8→4b); show tokens/s gains at long context. ([NeurIPS Papers](https://papers.nips.cc/paper_files/paper/2024/file/028fcbcf85435d39a40c4d61b42c99a4-Paper-Conference.pdf?utm_source=chatgpt.com "[PDF] KVQuant: Towards 10 Million Context Length LLM Inference with KV ..."))
    
- **M3:** Train a distilled student; then quantize it—plot the Pareto front vs. teacher-PTQ. ([Semantic Scholar](https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19?utm_source=chatgpt.com "Distilling the Knowledge in a Neural Network - Semantic Scholar"))
    
- **M4:** Port to TensorRT-LLM; compare FP8 vs. INT8 vs. W4A8; profile on one GPU. ([SemiAnalysis](https://semianalysis.com/2025/06/23/nvidia-tensor-core-evolution-from-volta-to-blackwell/?utm_source=chatgpt.com "NVIDIA Tensor Core Evolution: From Volta To Blackwell"))
    

# 8) Future prospects (where the field is headed)

- **Lower precisions with stability guards:** FP4 kernels + smarter calibration/rotation; mixed W4/A8/KV4 becoming standard on new GPUs. ([Hugging Face](https://huggingface.co/papers/2405.16406?utm_source=chatgpt.com "SpinQuant: LLM quantization with learned rotations - Hugging Face"), [arXiv](https://arxiv.org/abs/2404.00456?utm_source=chatgpt.com "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"))
    
- **Cache-centric optimization:** KV-cache quant + chunked attention will dominate long-context serving. ([NeurIPS Papers](https://papers.nips.cc/paper_files/paper/2024/file/028fcbcf85435d39a40c4d61b42c99a4-Paper-Conference.pdf?utm_source=chatgpt.com "[PDF] KVQuant: Towards 10 Million Context Length LLM Inference with KV ..."))
    
- **Hybrid compression stacks:** KD + quant + structured sparsity co-designed with kernels/compilers.
    
- **Compression-aware decoding:** merge compression with speculative/assisted decoding for cluster-scale wins. ([arXiv](https://arxiv.org/abs/2407.12391?utm_source=chatgpt.com "LLM Inference Serving: Survey of Recent Advances and Opportunities"))
    