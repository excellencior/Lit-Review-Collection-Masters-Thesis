This survey paper provides a comprehensive overview of techniques for **compressing LLMs** to enable efficient inference in resource-constrained environments.

# Primary Approaches
- Knowledge Distillation
- Model Quantization
- Model Pruning

**Supplementary Techniques**, i.e. mixture-of-experts, early-exit etc.

